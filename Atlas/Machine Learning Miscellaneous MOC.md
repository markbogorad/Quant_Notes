up:: [[Home]]
tags:: #MOC
# ML MOC
- Imputation -> mean/median
- Gradient Boosting Machines (GBM)
	- 
	- Gradient Boosting Algorithm
	- LightGBM model
- Random Forests (RF)
- Decision Trees (DT)
- Naive Bayes (NB)
- Gaussian Process (GP)
## Introduction
- [[Convergence of Probability]]
- [[Convergence of Distribution]]
- [[Law of Large Numbers]]
- [[Central Limit Theorem]]
- [[Bias Variance Tradeoff]]
- [[Maximum Likelihood Estimator]]
- [[Statistics vs Machine Learning]]
- [[Machine Learning Process]]
	- [[Preprocessing (ML)]]
	- [[ML Validation]]
	- [[Model Development]]
	- [[Explanations]]
- [[Machine Learning Applications]]
- [[Data Generating Processes (ML Techniques)]] -
- Assumptions for General ML
	- [[I.I.D.]]
	- Same distribution for training and test data
	- Distributions are fixed over time (stationary)
- Discrepancy Theory - when things are non forecastable
- ML Framework
	- Postulate a particular form of a parametric model that is assumed to generate data -> $f(\theta)$
		- making an assumption about historical data
	- Use the given examples to estimates unknown parameters of the model $\theta$ hat
		- Fine tune the assumptions (train)
	- use estimated model to make predictions $f(\theta)$ hat
		- make a prediction out of the fine tuning
	- example: AR models
		- 

## Function Spaces
- LP Space
- Holder Space
- Soboff Space
- [[Reproducing Kernel Hilbert Space]]

# HARD CMU course :
> Lecture 01: Review - YouTube](https://www.youtube.com/watch?v=zcMnu-3wkWo&list=PLTB9VQq8WiaCBK2XrtYn5t9uuPdsNm7YE&index=1
> 
## Other
- Concentration of Measure
	- Huftons Inequality
	- VC Theory
	- Radamacar Complexity
- Minimax Theory